{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Algorithm Improvement By Applying Bitmap Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import scipy as sp\n",
    "import numpy as np # Import numpy library \n",
    "from skimage.feature import hog # Import Hog model to extract features\n",
    "from sklearn.metrics import confusion_matrix # Import confusion matrix to evaluate the performance\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "y = []\n",
    "file_size = []\n",
    "k = 0\n",
    "path = \"./data/101_ObjectCategories\" # Give the dataset path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preprocessing:\n",
    "1. Load the images using cv2\n",
    "2. Image resize\n",
    "3. Feature extraction: BGR to Gray conversion \n",
    "4. Feature extraction: Histogram of Oriented Gradients(HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.listdir(path) # from the given path get the file names such as accordion, airplanes etc..\n",
    "for file in folder: # for every file name in the given path go inseide that directory and get the images\n",
    "    subpath = os.path.join(path,file)  # Join the name of these files to the previous path \n",
    "    \n",
    "    files = os.listdir(subpath) # Take these image names to a list called files\n",
    "    j = 0\n",
    "    for i in range(np.size(files)): # now we shall loop through these number of files\n",
    "        \n",
    "        im = cv2.imread(subpath+'/'+files[0+j]) # Read the images from this subpath\n",
    "        \n",
    "        imgs.append(im) # append all the read images to a list called imgs\n",
    "        y.append(k) # generate a labe to every file and append it to labels list\n",
    "\n",
    "        j += 1\n",
    "        if (j == (np.size(files))):\n",
    "            file_size.append(j)\n",
    "   \n",
    "    k += 1\n",
    "     \n",
    "y = np.array(y).tolist()\n",
    "ix = []\n",
    "for index, item in enumerate(imgs):\n",
    "    if (np.size(item) == 1):\n",
    "        ix.append(index)\n",
    "        del imgs[index]\n",
    "        \n",
    "for index, item in enumerate(y):\n",
    "    for v in range(np.size(ix)):\n",
    "        if (index == ix[v]):\n",
    "            del y[index]\n",
    "        \n",
    "y = np.array(y).astype(np.float64) \n",
    "\n",
    "# Function to convert an image from color to grayscale\n",
    "def resize_(image):\n",
    "    u = cv2.resize(image,(256,256))\n",
    "    return u\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def fd_hog(image):\n",
    "    fd = hog(image, orientations=8, pixels_per_cell=(64, 64),\n",
    "                        cells_per_block=(2, 2))\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (9176 of 9176) |####################| Elapsed Time: 0:01:55 Time:  0:01:55\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "import progressbar\n",
    "with progressbar.ProgressBar(max_value=len(imgs)) as bar:\n",
    "    i=1\n",
    "    for img in imgs:\n",
    "        b=resize_(img)\n",
    "        c=rgb2gray(b)   \n",
    "        d=fd_hog(c)\n",
    "        a.append(d)\n",
    "        bar.update(i)\n",
    "        i+=1\n",
    "df = pd.DataFrame(a)\n",
    "df['lable'] = y\n",
    "id_ = np.arange(1,len(df)+1,1)\n",
    "df['id'] = id_\n",
    "X = df.values\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    " .master(\"local\") \\\n",
    " .appName(\"Image Retrieval\") \\\n",
    " .config(\"spark.some.config.option\", \"some-value\") \\\n",
    " .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG diamension: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"HOG diamension: \")\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestPerformance(numOfTest, bucketLength,numHashTables,numOfNeighbor):\n",
    " \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "    Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])\n",
    "    Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "    Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])\n",
    "\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", \n",
    "                                      bucketLength=bucketLength,numHashTables=numHashTables)\n",
    "    \n",
    "    \n",
    "    model = brp.fit(Train_df)\n",
    "    model.transform(Train_df)\n",
    "  \n",
    "    nnToMajorityAccMap = {}\n",
    "    nnToWeightedAccMap = {}\n",
    "    with progressbar.ProgressBar(max_value = numOfTest) as bar:\n",
    "        for i in range(0, numOfTest):\n",
    "            Catg = X_test[i][-2]\n",
    "            key = Vectors.dense(X_test[i][0:-2])\n",
    "            # Choose the Last one of numOfNeighbor, the biggest one \n",
    "            result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor[-1], distCol=\"EuclideanDistance\")\n",
    "            # Conver pySpark framework colunm to python list\n",
    "            labelList = result.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "            # Build a distance dataframe for the distance between query and result\n",
    "            result1 = result.select('label', 'EuclideanDistance').collect()\n",
    "            df_weighted = pd.DataFrame()\n",
    "            df_weighted['Label'] = [int(row['label']) for row in result1]\n",
    "            df_weighted['EuclideanDistance'] = [float(row['EuclideanDistance']) for row in result1]\n",
    "            df_weighted['Weight'] = 1 / df_weighted['EuclideanDistance']\n",
    "            # slice LableList into differnt length subLists \n",
    "            nnList = []\n",
    "            for numberNN in numOfNeighbor:\n",
    "                slicedList = labelList[0:numberNN]\n",
    "                nnList.append(slicedList)\n",
    "                \n",
    "            for index in range(0, len(nnList)):\n",
    "                majority_vote = Counter(nnList[index]).most_common(1)[0][0]\n",
    "                if  Catg == majority_vote:\n",
    "                    key = numOfNeighbor[index]\n",
    "                    if key in nnToMajorityAccMap:\n",
    "                        nnToMajorityAccMap[key] = nnToMajorityAccMap.get(key) + 1\n",
    "                    else:\n",
    "                        nnToMajorityAccMap[key] = 1\n",
    "                        \n",
    "            for n in numOfNeighbor:\n",
    "                weighted_result = df_weighted.head(n).groupby('Label')['Weight'].sum()\n",
    "                df_weighted_result = pd.DataFrame(weighted_result)\n",
    "                df_weighted_result = df_weighted_result.reset_index()\n",
    "                df_weighted_result.sort_values(by='Weight',inplace=True,ascending=False)\n",
    "                weighted_vote = df_weighted_result.iloc[0]['Label'].astype('int')\n",
    "                if  Catg == weighted_vote:\n",
    "                    key = n\n",
    "                    if key in nnToWeightedAccMap:\n",
    "                        nnToWeightedAccMap[key] = nnToWeightedAccMap.get(key) + 1\n",
    "                    else:\n",
    "                        nnToWeightedAccMap[key] = 1\n",
    "                        \n",
    "            bar.update(i)\n",
    "        # calucate accuracy base on Majority Vote\n",
    "        for key in nnToMajorityAccMap:\n",
    "            nnToMajorityAccMap[key] = nnToMajorityAccMap.get(key) / numOfTest\n",
    "        # calucate accuracy base on Weighted Vote\n",
    "        for key in nnToWeightedAccMap:\n",
    "            nnToWeightedAccMap[key] = nnToWeightedAccMap.get(key) / numOfTest    \n",
    "    return nnToMajorityAccMap, nnToWeightedAccMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking bucketLength Param:\n",
      "[ 1 10 20 30 40]\n",
      "Checking numHashTablesList Param:\n",
      "[  1  30  60  90 120]\n"
     ]
    }
   ],
   "source": [
    "#set Param \n",
    "bucketLengthList = np.arange(0, 50, 10)\n",
    "numHashTablesList = np.arange(0, 150, 30)\n",
    "bucketLengthList[0] = 1\n",
    "numHashTablesList[0] = 1\n",
    "#make sure the last element of numOfNeighborList is the largest\n",
    "numOfNeighborList = [1, 3, 5, 7, 15, 21, 25];\n",
    "numOfTest = 100\n",
    "print(\"Checking bucketLength Param:\")\n",
    "print(bucketLengthList)\n",
    "print(\"Checking numHashTablesList Param:\")\n",
    "print(numHashTablesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (100 of 100) |######################| Elapsed Time: 0:02:51 Time:  0:02:51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:1  Majority Vote Acc: {1: 0.43, 3: 0.44, 5: 0.43, 7: 0.45, 15: 0.45, 21: 0.42, 25: 0.43}\n",
      "bucketLen:1  #Hashtable:1  Weighted Vote Acc: {1: 0.43, 3: 0.44, 5: 0.44, 7: 0.46, 15: 0.47, 21: 0.42, 25: 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "N/A% (0 of 100) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    }
   ],
   "source": [
    "bucketLengthList_para=[]\n",
    "numHashTablesList_para=[]\n",
    "resultMList = []\n",
    "resultWList = []\n",
    "for i in bucketLengthList:\n",
    "    for j in numHashTablesList:\n",
    "            resultM, resultW = getBestPerformance(numOfTest ,i, j, numOfNeighborList)\n",
    "            print( \"bucketLen:\" + str(i) + \"  #Hashtable:\" + str(j) +  \"  Majority Vote Acc: \" + str(resultM))\n",
    "            print( \"bucketLen:\" + str(i) + \"  #Hashtable:\" + str(j) +  \"  Weighted Vote Acc: \" + str(resultW))\n",
    "            bucketLengthList_para.append(i)\n",
    "            numHashTablesList_para.append(j)\n",
    "            resultMList.append(resultM)\n",
    "            resultWList.append(resultW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['BucketLength'] = bucketLengthList_para\n",
    "df_result['NumHashTables'] = numHashTablesList_para\n",
    "# conver List of map to panda dataframework\n",
    "df_result_map = pd.DataFrame(resultMList)\n",
    "df_result_map2 = pd.DataFrame(resultWList)\n",
    "df_result = pd.concat([df_result, df_result_map], axis=1, join='inner')\n",
    "df_result = pd.concat([df_result, df_result_map2], axis=1, join='inner')\n",
    "\n",
    "# df_result[\"Acc\"] = resultList\n",
    "# df_result = df_result.sort_values(by=['Acc'],ascending=False)\n",
    "df_result.to_csv('./result2.csv') #Chang the name every you wanna sava a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!Skip all the code below !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Split the data to training and validation data. We choose 70% for training and 30% for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 428 ms, total: 2.16 s\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# append 'label' and 'id' to the last two colunms\n",
    "df = pd.DataFrame(a)\n",
    "df['lable'] = y\n",
    "id_ = np.arange(1,len(df)+1,1)\n",
    "df['id'] = id_\n",
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PySpark to retrieve similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Image Retrieval\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------------------+\n",
      "|  id|label|            features|\n",
      "+----+-----+--------------------+\n",
      "|5960|   80|[0.00363684489770...|\n",
      "|1536|   18|[0.06071563370892...|\n",
      "+----+-----+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Train_df.show(n = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+----+-----+--------------------+--------------------+\n",
      "|  id|label|            features|              hashes|\n",
      "+----+-----+--------------------+--------------------+\n",
      "|5960|   80|[0.00363684489770...|[[0.0], [0.0], [-...|\n",
      "|1536|   18|[0.06071563370892...|[[0.0], [0.0], [-...|\n",
      "|8387|   95|[0.01072387506065...|[[-1.0], [0.0], [...|\n",
      "|6169|   83|[0.00663216869878...|[[0.0], [0.0], [0...|\n",
      "|4067|   55|[0.01922287915132...|[[-1.0], [-1.0], ...|\n",
      "|2135|   23|[0.02042261440886...|[[0.0], [0.0], [0...|\n",
      "|8246|   95|[0.02765804852049...|[[-1.0], [-1.0], ...|\n",
      "|8690|   96|[0.03719083421779...|[[-1.0], [0.0], [...|\n",
      "|6724|   88|[0.06854134811831...|[[-1.0], [0.0], [...|\n",
      "|  26|    0|[0.04798466879403...|[[-1.0], [0.0], [...|\n",
      "|6432|   88|[0.01443607483278...|[[0.0], [0.0], [-...|\n",
      "|2392|   28|[0.02645665091604...|[[-1.0], [0.0], [...|\n",
      "|1580|   18|[0.16710970919873...|[[-1.0], [0.0], [...|\n",
      "|1388|   18|[0.04740743977419...|[[0.0], [0.0], [-...|\n",
      "| 914|   16|[0.03339826134034...|[[0.0], [0.0], [0...|\n",
      "| 222|    4|[0.05095278145434...|[[-1.0], [0.0], [...|\n",
      "|1594|   18|[0.01278881740856...|[[0.0], [0.0], [-...|\n",
      "|6710|   88|[0.05449112230859...|[[-1.0], [0.0], [...|\n",
      "|4701|   62|[0.05434406875582...|[[0.0], [0.0], [-...|\n",
      "|6126|   82|[0.03130979499120...|[[-1.0], [-1.0], ...|\n",
      "+----+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "peak memory: 832.16 MiB, increment: 0.08 MiB\n",
      "CPU times: user 112 ms, sys: 42.7 ms, total: 155 ms\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%reload_ext memory_profiler\n",
    "def toMeasureMemo():\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",bucketLength=2,numHashTables=3)\n",
    "    model = brp.fit(Train_df)\n",
    "    print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "    model.transform(Train_df).show()\n",
    "%memit  toMeasureMemo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Vectors.dense(X_test[0][0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately searching Train_df for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(Train_df, key, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_id = result.select('label',).collect()\n",
    "# result_id[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Approximately joining Train_df and Test_df on Euclidean distance smaller than 1:\")\n",
    "# model.approxSimilarityJoin(Train_df, Test_df, 1.1, distCol=\"EuclideanDistance\")\\\n",
    "#     .select(col(\"datasetA.id\").alias(\"Train_df\"),\n",
    "#             col(\"datasetB.id\").alias(\"Test_df\"),\n",
    "#             col(\"EuclideanDistance\")).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "numOfNeighbor = 5\n",
    "numOfTest= 5\n",
    "accList = []\n",
    "with progressbar.ProgressBar(max_value=numOfTest) as bar:\n",
    "    for i in range(0, numOfTest):\n",
    "        Catg = X_test[i][-2]\n",
    "        key = Vectors.dense(X_test[i][0:-2])\n",
    "        result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor)\n",
    "        temp = Counter([int(row['label']) for row in result.collect()])\n",
    "        if  Catg in temp:\n",
    "            accuracy += temp.get(Catg)/ numOfNeighbor\n",
    "            accList.append(temp.get(Catg)/ numOfNeighbor)\n",
    "        else:\n",
    "            accList.append(0)\n",
    "        bar.update(i)\n",
    "    accuracy /= numOfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import imshow\n",
    "# imshow(imgs[4795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n",
      "peak memory: 288.71 MiB, increment: 160.33 MiB\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler\n",
    "def my_func():\n",
    "    a = [1] * (10 **6)\n",
    "    b = [2] * (2 * 10 ** 7)\n",
    "    del b\n",
    "    return a\n",
    "%memit my_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [1, 2, 3], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6, 7]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
