{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Algorithm Improvement By Applying Bitmap Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar2 in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (3.47.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (from progressbar2) (2.3.0)\n",
      "Requirement already satisfied: six in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (from progressbar2) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install progressbar2\n",
    "import argparse\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import scipy as sp\n",
    "import numpy as np # Import numpy library \n",
    "from skimage.feature import hog # Import Hog model to extract features\n",
    "from sklearn.metrics import confusion_matrix # Import confusion matrix to evaluate the performance\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "y = []\n",
    "file_size = []\n",
    "k = 0\n",
    "path = \"./data/101_ObjectCategories\" # Give the dataset path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preprocessing:\n",
    "1. Load the images using cv2\n",
    "2. Image resize\n",
    "3. Feature extraction: BGR to Gray conversion \n",
    "4. Feature extraction: Histogram of Oriented Gradients(HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.listdir(path) # from the given path get the file names such as accordion, airplanes etc..\n",
    "for file in folder: # for every file name in the given path go inseide that directory and get the images\n",
    "    subpath = os.path.join(path,file)  # Join the name of these files to the previous path \n",
    "    \n",
    "    files = os.listdir(subpath) # Take these image names to a list called files\n",
    "    j = 0\n",
    "    for i in range(np.size(files)): # now we shall loop through these number of files\n",
    "        \n",
    "        im = cv2.imread(subpath+'/'+files[0+j]) # Read the images from this subpath\n",
    "        \n",
    "        imgs.append(im) # append all the read images to a list called imgs\n",
    "        y.append(k) # generate a labe to every file and append it to labels list\n",
    "\n",
    "        j += 1\n",
    "        if (j == (np.size(files))):\n",
    "            file_size.append(j)\n",
    "   \n",
    "    k += 1\n",
    "     \n",
    "y = np.array(y).tolist()\n",
    "ix = []\n",
    "for index, item in enumerate(imgs):\n",
    "    if (np.size(item) == 1):\n",
    "        ix.append(index)\n",
    "        del imgs[index]\n",
    "        \n",
    "for index, item in enumerate(y):\n",
    "    for v in range(np.size(ix)):\n",
    "        if (index == ix[v]):\n",
    "            del y[index]\n",
    "        \n",
    "y = np.array(y).astype(np.float64) \n",
    "\n",
    "# Function to convert an image from color to grayscale\n",
    "def resize_(image):\n",
    "    u = cv2.resize(image,(256,256))\n",
    "    return u\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def fd_hog(image):\n",
    "    fd = hog(image, orientations=8, pixels_per_cell=(64, 64),\n",
    "                        cells_per_block=(2, 2))\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fudonghuang/anaconda3/lib/python3.7/site-packages/skimage/feature/_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n",
      "100% (9176 of 9176) |####################| Elapsed Time: 0:03:46 Time:  0:03:46\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "import progressbar\n",
    "with progressbar.ProgressBar(max_value=len(imgs)) as bar:\n",
    "    i=1\n",
    "    for img in imgs:\n",
    "        b=resize_(img)\n",
    "        c=rgb2gray(b)   \n",
    "        d=fd_hog(c)\n",
    "        a.append(d)\n",
    "        bar.update(i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG diamension: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"HOG diamension: \")\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestPerformance(a, numOfTest, bucketLength,numHashTables,numOfNeighbor):\n",
    "    df = pd.DataFrame(a)\n",
    "    df['lable'] = y\n",
    "    id_ = np.arange(1,len(df)+1,1)\n",
    "    df['id'] = id_\n",
    "    X = df.values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Image Retrieval\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "    Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "    Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])\n",
    "    Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "    Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])\n",
    "\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", \n",
    "                                      bucketLength=bucketLength,numHashTables=numHashTables)\n",
    "    \n",
    "    \n",
    "    model = brp.fit(Train_df)\n",
    "    model.transform(Train_df)\n",
    "  \n",
    "    nnToAccMap = {}\n",
    "    with progressbar.ProgressBar(max_value = numOfTest) as bar:\n",
    "        for i in range(0, numOfTest):\n",
    "            Catg = X_test[i][-2]\n",
    "            key = Vectors.dense(X_test[i][0:-2])\n",
    "            # Choose the Last one of numOfNeighbor, the biggest one \n",
    "            result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor[-1])\n",
    "            # Conver pySpark framework colunm to python list\n",
    "            labelList = result.select(\"label\").rdd.flatMap(lambda x: x).collect()\n",
    "            # slice LableList into differnt length subLists \n",
    "            nnList = []\n",
    "            for numberNN in numOfNeighbor:\n",
    "                slicedList = labelList[0:numberNN]\n",
    "                nnList.append(slicedList)\n",
    "                \n",
    "            for index in range(0, len(nnList)):\n",
    "                majority_vote = Counter(nnList[index]).most_common(1)[0][0]\n",
    "                if  Catg == majority_vote:\n",
    "                    key = numOfNeighbor[index]\n",
    "                    if key in nnToAccMap:\n",
    "                        nnToAccMap[key] = nnToAccMap.get(key) + 1\n",
    "                    else:\n",
    "                        nnToAccMap[key] = 1\n",
    "            bar.update(i)\n",
    "        # calucate accuracy\n",
    "        for key in nnToAccMap:\n",
    "            nnToAccMap[key] = nnToAccMap.get(key) / numOfTest\n",
    "    return nnToAccMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking bucketLength Param:\n",
      "[ 1 10 20 30 40]\n",
      "Checking numHashTablesList Param:\n",
      "[  1  30  60  90 120]\n"
     ]
    }
   ],
   "source": [
    "#set Param \n",
    "bucketLengthList = np.arange(0, 50, 10)\n",
    "numHashTablesList = np.arange(0, 150, 30)\n",
    "bucketLengthList[0] = 1\n",
    "numHashTablesList[0] = 1\n",
    "#make sure the last element of numOfNeighborList is the largest\n",
    "numOfNeighborList = [1, 3, 5, 7, 15, 21, 25];\n",
    "numOfTest = 1000\n",
    "print(\"Checking bucketLength Param:\")\n",
    "print(bucketLengthList)\n",
    "print(\"Checking numHashTablesList Param:\")\n",
    "print(numHashTablesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:18:09 Time:  0:18:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:1 Acc: {1: 0.456, 3: 0.463, 5: 0.457, 7: 0.457, 15: 0.452, 21: 0.44, 25: 0.428}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:24:53 Time:  0:24:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:30 Acc: {1: 0.503, 3: 0.51, 5: 0.514, 7: 0.501, 15: 0.487, 21: 0.471, 25: 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:20:32 Time:  0:20:32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:60 Acc: {1: 0.491, 3: 0.492, 5: 0.501, 7: 0.498, 15: 0.483, 21: 0.478, 25: 0.478}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:38:42 Time:  0:38:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:90 Acc: {1: 0.498, 3: 0.503, 5: 0.501, 7: 0.494, 15: 0.494, 21: 0.484, 25: 0.472}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:41:19 Time:  0:41:19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:1  #Hashtable:120 Acc: {7: 0.477, 25: 0.456, 1: 0.471, 3: 0.479, 5: 0.481, 15: 0.465, 21: 0.464}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:14:54 Time:  0:14:54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:10  #Hashtable:1 Acc: {1: 0.474, 3: 0.476, 5: 0.483, 15: 0.456, 21: 0.442, 25: 0.433, 7: 0.468}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:20:58 Time:  0:20:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:10  #Hashtable:30 Acc: {1: 0.474, 3: 0.471, 5: 0.474, 7: 0.468, 15: 0.455, 21: 0.449, 25: 0.443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:25:45 Time:  0:25:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:10  #Hashtable:60 Acc: {1: 0.49, 3: 0.493, 5: 0.483, 7: 0.478, 15: 0.458, 21: 0.451, 25: 0.446}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:20:03 Time:  0:20:03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-f1be992bad65>\u001b[0m in \u001b[0;36mgetBestPerformance\u001b[0;34m(a, numOfTest, bucketLength, numHashTables, numOfNeighbor)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxNearestNeighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumOfNeighbor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Conver pySpark framework colunm to python list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mlabelList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mnnList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnumberNN\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumOfNeighbor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucketLengthList_para=[]\n",
    "numHashTablesList_para=[]\n",
    "resultList = []\n",
    "for i in bucketLengthList:\n",
    "    for j in numHashTablesList:\n",
    "            result = getBestPerformance(a, numOfTest ,i, j, numOfNeighborList)\n",
    "            print( \"bucketLen:\" + str(i) + \"  #Hashtable:\" + str(j) +  \" Acc: \" + str(result))\n",
    "            bucketLengthList_para.append(i)\n",
    "            numHashTablesList_para.append(j)\n",
    "            resultList.append(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['BucketLength'] = bucketLengthList_para\n",
    "df_result['NumHashTables'] = numHashTablesList_para\n",
    "# conver List of map to panda dataframework\n",
    "df_result_map = pd.DataFrame(resultList)\n",
    "df_result = pd.concat([df_result, df_result_map], axis=1, join='inner')\n",
    "\n",
    "# df_result[\"Acc\"] = resultList\n",
    "# df_result = df_result.sort_values(by=['Acc'],ascending=False)\n",
    "df_result.to_csv('./result2.csv') #Chang the name every you wanna sava a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BucketLength</th>\n",
       "      <th>NumHashTables</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>15</th>\n",
       "      <th>21</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.471</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   BucketLength  NumHashTables      1      3      5      7     15     21  \\\n",
       "0             1              1  0.456  0.463  0.457  0.457  0.452  0.440   \n",
       "1             1             30  0.503  0.510  0.514  0.501  0.487  0.471   \n",
       "2             1             60  0.491  0.492  0.501  0.498  0.483  0.478   \n",
       "3             1             90  0.498  0.503  0.501  0.494  0.494  0.484   \n",
       "4             1            120  0.471  0.479  0.481  0.477  0.465  0.464   \n",
       "5            10              1  0.474  0.476  0.483  0.468  0.456  0.442   \n",
       "6            10             30  0.474  0.471  0.474  0.468  0.455  0.449   \n",
       "7            10             60  0.490  0.493  0.483  0.478  0.458  0.451   \n",
       "\n",
       "      25  \n",
       "0  0.428  \n",
       "1  0.460  \n",
       "2  0.478  \n",
       "3  0.472  \n",
       "4  0.456  \n",
       "5  0.433  \n",
       "6  0.443  \n",
       "7  0.446  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!Skip all the code below !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Split the data to training and validation data. We choose 70% for training and 30% for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# append 'label' and 'id' to the last two colunms\n",
    "df = pd.DataFrame(a)\n",
    "df['lable'] = y\n",
    "id_ = np.arange(1,len(df)+1,1)\n",
    "df['id'] = id_\n",
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PySpark to retrieve similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Image Retrieval\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.show(n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!! Skip以下代码直接运行最后一行 !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## skip以下代码直接运行最后一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",bucketLength=2,numHashTables=3)\n",
    "model = brp.fit(Train_df)\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(Train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Vectors.dense(X_test[0][0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately searching Train_df for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(Train_df, key, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_id = result.select('label',).collect()\n",
    "# result_id[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Approximately joining Train_df and Test_df on Euclidean distance smaller than 1:\")\n",
    "# model.approxSimilarityJoin(Train_df, Test_df, 1.1, distCol=\"EuclideanDistance\")\\\n",
    "#     .select(col(\"datasetA.id\").alias(\"Train_df\"),\n",
    "#             col(\"datasetB.id\").alias(\"Test_df\"),\n",
    "#             col(\"EuclideanDistance\")).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "numOfNeighbor = 5\n",
    "numOfTest= 5\n",
    "accList = []\n",
    "with progressbar.ProgressBar(max_value=numOfTest) as bar:\n",
    "    for i in range(0, numOfTest):\n",
    "        Catg = X_test[i][-2]\n",
    "        key = Vectors.dense(X_test[i][0:-2])\n",
    "        result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor)\n",
    "        temp = Counter([int(row['label']) for row in result.collect()])\n",
    "        if  Catg in temp:\n",
    "            accuracy += temp.get(Catg)/ numOfNeighbor\n",
    "            accList.append(temp.get(Catg)/ numOfNeighbor)\n",
    "        else:\n",
    "            accList.append(0)\n",
    "        bar.update(i)\n",
    "    accuracy /= numOfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行下面一行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import imshow\n",
    "# imshow(imgs[4795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
