{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSH Algorithm Improvement By Applying Bitmap Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: progressbar2 in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (3.47.0)\n",
      "Requirement already satisfied: python-utils>=2.3.0 in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (from progressbar2) (2.3.0)\n",
      "Requirement already satisfied: six in /Users/fudonghuang/anaconda3/lib/python3.7/site-packages (from progressbar2) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install progressbar2\n",
    "import argparse\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import imagehash\n",
    "from PIL import Image\n",
    "import os, os.path\n",
    "import cv2\n",
    "from collections import Counter\n",
    "import scipy as sp\n",
    "import numpy as np # Import numpy library \n",
    "from skimage.feature import hog # Import Hog model to extract features\n",
    "from sklearn.metrics import confusion_matrix # Import confusion matrix to evaluate the performance\n",
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "y = []\n",
    "file_size = []\n",
    "k = 0\n",
    "path = \"./data/101_ObjectCategories\" # Give the dataset path here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data Preprocessing:\n",
    "1. Load the images using cv2\n",
    "2. Image resize\n",
    "3. Feature extraction: BGR to Gray conversion \n",
    "4. Feature extraction: Histogram of Oriented Gradients(HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.listdir(path) # from the given path get the file names such as accordion, airplanes etc..\n",
    "for file in folder: # for every file name in the given path go inseide that directory and get the images\n",
    "    subpath = os.path.join(path,file)  # Join the name of these files to the previous path \n",
    "    \n",
    "    files = os.listdir(subpath) # Take these image names to a list called files\n",
    "    j = 0\n",
    "    for i in range(np.size(files)): # now we shall loop through these number of files\n",
    "        \n",
    "        im = cv2.imread(subpath+'/'+files[0+j]) # Read the images from this subpath\n",
    "        \n",
    "        imgs.append(im) # append all the read images to a list called imgs\n",
    "        y.append(k) # generate a labe to every file and append it to labels list\n",
    "\n",
    "        j += 1\n",
    "        if (j == (np.size(files))):\n",
    "            file_size.append(j)\n",
    "   \n",
    "    k += 1\n",
    "     \n",
    "y = np.array(y).tolist()\n",
    "ix = []\n",
    "for index, item in enumerate(imgs):\n",
    "    if (np.size(item) == 1):\n",
    "        ix.append(index)\n",
    "        del imgs[index]\n",
    "        \n",
    "for index, item in enumerate(y):\n",
    "    for v in range(np.size(ix)):\n",
    "        if (index == ix[v]):\n",
    "            del y[index]\n",
    "        \n",
    "y = np.array(y).astype(np.float64) \n",
    "\n",
    "# Function to convert an image from color to grayscale\n",
    "def rgb2gray(rgb):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def resize_(image):\n",
    "    u = cv2.resize(image,(256,256))\n",
    "    return u\n",
    "\n",
    "def fd_hog(image):\n",
    "    fd = hog(image, orientations=8, pixels_per_cell=(64, 64),\n",
    "                        cells_per_block=(2, 2))\n",
    "    \n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fudonghuang/anaconda3/lib/python3.7/site-packages/skimage/feature/_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n",
      "100% (9176 of 9176) |####################| Elapsed Time: 0:04:05 Time:  0:04:05\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "import progressbar\n",
    "with progressbar.ProgressBar(max_value=len(imgs)) as bar:\n",
    "    i=1\n",
    "    for img in imgs:\n",
    "        b=resize_(img)\n",
    "        c=rgb2gray(b)   \n",
    "        d=fd_hog(c)\n",
    "        a.append(d)\n",
    "        bar.update(i)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOG diamension: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "288"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"HOG diamension: \")\n",
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestPerformance(a, numOfTest, bucketLength,numHashTables,numOfNeighbor):\n",
    "    df = pd.DataFrame(a)\n",
    "    df['lable'] = y\n",
    "    id_ = np.arange(1,len(df)+1,1)\n",
    "    df['id'] = id_\n",
    "    X = df.values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Image Retrieval\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()\n",
    "\n",
    "    Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "    Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])\n",
    "    Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "    Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])\n",
    "\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", \n",
    "                                      bucketLength=bucketLength,numHashTables=numHashTables)\n",
    "    \n",
    "    \n",
    "    model = brp.fit(Train_df)\n",
    "    model.transform(Train_df)\n",
    "    ## run test dataset \n",
    "    accuracy = 0\n",
    "    with progressbar.ProgressBar(max_value = numOfTest) as bar:\n",
    "        for i in range(0, numOfTest):\n",
    "            Catg = X_test[i][-2]\n",
    "            key = Vectors.dense(X_test[i][0:-2])\n",
    "            result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor)\n",
    "            temp = Counter([int(row['label']) for row in result.collect()])\n",
    "            if  Catg in temp:\n",
    "                accuracy += temp.get(Catg)/ numOfNeighbor\n",
    "            bar.update(i)\n",
    "        accuracy /= numOfTest\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking bucketLength Param:\n",
      "[20 25 30 35 40 45 50 55 60]\n",
      "Checking numHashTablesList Param:\n",
      "[  1  10  20  30  40  50  60  70  80  90 100 110 120]\n"
     ]
    }
   ],
   "source": [
    "#set Param \n",
    "bucketLengthList = np.arange(20, 61, 5)\n",
    "numHashTablesList = np.arange(0,125,10)\n",
    "numHashTablesList[0] = 1\n",
    "numOfNeighbor = [5]\n",
    "numOfTest = 1000\n",
    "print(\"Checking bucketLength Param:\")\n",
    "print(bucketLengthList)\n",
    "print(\"Checking numHashTablesList Param:\")\n",
    "print(numHashTablesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:17:50 Time:  0:17:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:1  #Neighbor:5  Acc:0.39259999999999917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:15:14 Time:  0:15:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:10  #Neighbor:5  Acc:0.41099999999999925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:14:49 Time:  0:14:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:20  #Neighbor:5  Acc:0.425399999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:15:46 Time:  0:15:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:30  #Neighbor:5  Acc:0.4125999999999991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:19:06 Time:  0:19:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:40  #Neighbor:5  Acc:0.4037999999999989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:26:49 Time:  0:26:49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:50  #Neighbor:5  Acc:0.4129999999999993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1000 of 1000) |####################| Elapsed Time: 0:31:00 Time:  0:31:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketLen:20  #Hashtable:60  #Neighbor:5  Acc:0.41559999999999914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49% (493 of 1000) |##########           | Elapsed Time: 0:12:14 ETA:   0:13:36"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bucketLengthList_para=[]\n",
    "numHashTablesList_para=[]\n",
    "resList = []\n",
    "for i in bucketLengthList:\n",
    "    for j in numHashTablesList:\n",
    "        for k in numOfNeighbor:\n",
    "            result = getBestPerformance(a, numOfTest ,i, j, k)\n",
    "            resList.append(result)\n",
    "            resStr = \"bucketLen:\" + str(i) + \"  #Hashtable:\" + str(j) + \"  #Neighbor:\" + str(k) + \"  Acc:\" + str(result)\n",
    "            print(resStr)\n",
    "            bucketLengthList_para.append(i)\n",
    "            numHashTablesList_para.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.DataFrame()\n",
    "df_result['BucketLength'] = bucketLengthList_para\n",
    "df_result['NumHashTables'] = numHashTablesList_para\n",
    "df_result['Acc'] = resList\n",
    "df_result = df_result.sort_values(by=['Acc'],ascending=False)\n",
    "df_result.to_csv('./result.csv') #Chang the name every you wanna sava a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!Skip all the code below !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "Split the data to training and validation data. We choose 70% for training and 30% for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# append 'label' and 'id' to the last two colunms\n",
    "df = pd.DataFrame(a)\n",
    "df['lable'] = y\n",
    "id_ = np.arange(1,len(df)+1,1)\n",
    "df['id'] = id_\n",
    "X = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PySpark to retrieve similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "     .master(\"local\") \\\n",
    "     .appName(\"Image Retrieval\") \\\n",
    "     .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_train)\n",
    "Train_df = spark.createDataFrame(Train,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test = map(lambda x: (int(x[-1]),int(x[-2]),Vectors.dense(x[:-2])), X_test)\n",
    "Test_df = spark.createDataFrame(Test,schema=['id','label',\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_df.show(n = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!!!! Skip以下代码直接运行最后一行 !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## skip以下代码直接运行最后一行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",bucketLength=2,numHashTables=3)\n",
    "model = brp.fit(Train_df)\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(Train_df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = Vectors.dense(X_test[0][0:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Approximately searching Train_df for 2 nearest neighbors of the key:\")\n",
    "model.approxNearestNeighbors(Train_df, key, 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_id = result.select('label',).collect()\n",
    "# result_id[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Approximately joining Train_df and Test_df on Euclidean distance smaller than 1:\")\n",
    "# model.approxSimilarityJoin(Train_df, Test_df, 1.1, distCol=\"EuclideanDistance\")\\\n",
    "#     .select(col(\"datasetA.id\").alias(\"Train_df\"),\n",
    "#             col(\"datasetB.id\").alias(\"Test_df\"),\n",
    "#             col(\"EuclideanDistance\")).show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = 0\n",
    "numOfNeighbor = 5\n",
    "numOfTest= 5\n",
    "accList = []\n",
    "with progressbar.ProgressBar(max_value=numOfTest) as bar:\n",
    "    for i in range(0, numOfTest):\n",
    "        Catg = X_test[i][-2]\n",
    "        key = Vectors.dense(X_test[i][0:-2])\n",
    "        result = model.approxNearestNeighbors(Train_df, key, numOfNeighbor)\n",
    "        temp = Counter([int(row['label']) for row in result.collect()])\n",
    "        if  Catg in temp:\n",
    "            accuracy += temp.get(Catg)/ numOfNeighbor\n",
    "            accList.append(temp.get(Catg)/ numOfNeighbor)\n",
    "        else:\n",
    "            accList.append(0)\n",
    "        bar.update(i)\n",
    "    accuracy /= numOfTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 运行下面一行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import imshow\n",
    "# imshow(imgs[4795])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
