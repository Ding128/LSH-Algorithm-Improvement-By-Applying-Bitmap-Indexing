{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the necessary libraries\n",
    "\n",
    "import os, os.path\n",
    "import cv2 # important : import Open CV\n",
    "import numpy as np # Import numpy library \n",
    "from sklearn.model_selection import StratifiedKFold #Import stratified kfold as we are using a 10fold cross validation approach\n",
    "from sklearn.svm import LinearSVC # Import the Linear SVM Classifier\n",
    "from skimage.feature import hog # Import Hog model to extract features\n",
    "from sklearn.metrics import confusion_matrix # Import confusion matrix to evaluate the performance\n",
    "\n",
    "# create the necessary empty lists\n",
    "imgs = []\n",
    "y=[]\n",
    "file_size = []\n",
    "k=0\n",
    "path = \"./data/101_ObjectCategories\" # Give the dataset path here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.listdir(path) # from the given path get the file names such as accordion, airplanes etc..\n",
    "for file in folder: # for every file name in the given path go inseide that directory and get the images\n",
    "    subpath = os.path.join(path,file)  # Join the name of these files to the previous path \n",
    "    \n",
    "    files = os.listdir(subpath) # Take these image names to a list called files\n",
    "    j=0\n",
    "    for i in range(np.size(files)): # now we shall loop through these number of files\n",
    "        \n",
    "        im=cv2.imread(subpath+'/'+files[0+j]) # Read the images from this subpath\n",
    "        \n",
    "        imgs.append(im) # append all the read images to a list called imgs\n",
    "        y.append(k) # generate a labe to every file and append it to y\n",
    "\n",
    "        j+=1\n",
    "        if (j == (np.size(files))):\n",
    "            file_size.append(j)\n",
    "   \n",
    "    k+=1\n",
    "     \n",
    "y=np.array(y).tolist()\n",
    "\n",
    "ix=[]\n",
    "for index, item in enumerate(imgs):\n",
    "    if (np.size(item) == 1):\n",
    "        ix.append(index)\n",
    "        del imgs[index]\n",
    "        \n",
    "for index, item in enumerate(y):\n",
    "    for v in range(np.size(ix)):\n",
    "        if (index == ix[v]):\n",
    "            del y[index]\n",
    "        \n",
    "y=np.array(y).astype(np.float64) \n",
    "\n",
    "# Function to convert an image from color to grayscale\n",
    "def rgb2gray(rgb):\n",
    "    gray = cv2.cvtColor(rgb, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def resize_(image):\n",
    "    u=cv2.resize(image,(256,256))\n",
    "    return u\n",
    "\n",
    "def fd_hog(image):\n",
    "    fd, hog_image = hog(image, orientations=8, pixels_per_cell=(64, 64),\n",
    "                    cells_per_block=(1, 1), visualise=True)\n",
    "    \n",
    "    return fd\n",
    "\n",
    "a=[]\n",
    "\n",
    "for img in imgs:\n",
    "    \n",
    "    b=resize_(img)\n",
    "    c=rgb2gray(b)   \n",
    "    d=fd_hog(c)\n",
    "    a.append(d)\n",
    "\n",
    "a=np.array(a)\n",
    "\n",
    "score = []\n",
    "skf = StratifiedKFold(n_splits = 10) # divide the complete dataset in to 10 folds\n",
    "skf.get_n_splits(a, y)\n",
    "\n",
    "accuracy = 0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(a,y):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = a[train_index], a[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index] \n",
    "\n",
    "\n",
    "    clf = LinearSVC(random_state=0)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    scr = clf.score(X_test, y_test)\n",
    "    print(scr)\n",
    "    score.append(scr)\n",
    "    accuracy += scr\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(cnf_matrix)\n",
    "\n",
    "score = np.array(score)\n",
    "\n",
    "fin_accuracy = (accuracy/10)*100\n",
    "\n",
    "print(\"Final Accuracy is: {}\".format(fin_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hashlib import sha1\n",
    "import numpy as np\n",
    "from datasketch.minhash import MinHash\n",
    "from datasketch.weighted_minhash import WeightedMinHashGenerator\n",
    "from datasketch.lsh import MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'datasets'])\n",
    "set2 = set(['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "set3 = set(['minhash', 'is', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.random.uniform(1, 10, 10)\n",
    "v2 = np.random.uniform(1, 10, 10)\n",
    "v3 = np.random.uniform(1, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg1():\n",
    "    m1 = MinHash(num_perm=128)\n",
    "    m2 = MinHash(num_perm=128)\n",
    "    m3 = MinHash(num_perm=128)\n",
    "    for d in set1:\n",
    "        m1.update(d.encode('utf8'))\n",
    "    for d in set2:\n",
    "        m2.update(d.encode('utf8'))\n",
    "    for d in set3:\n",
    "        m3.update(d.encode('utf8'))\n",
    "\n",
    "    # Create LSH index\n",
    "    lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "    lsh.insert(\"m2\", m2)\n",
    "    lsh.insert(\"m3\", m3)\n",
    "    result = lsh.query(m1)\n",
    "    print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eg2():\n",
    "    mg = WeightedMinHashGenerator(10, 5)\n",
    "    m1 = mg.minhash(v1)\n",
    "    m2 = mg.minhash(v2)\n",
    "    m3 = mg.minhash(v3)\n",
    "    print(\"Estimated Jaccard m1, m2\", m1.jaccard(m2))\n",
    "    print(\"Estimated Jaccard m1, m3\", m1.jaccard(m3))\n",
    "    # Create LSH index\n",
    "    lsh = MinHashLSH(threshold=0.1, num_perm=5)\n",
    "    lsh.insert(\"m2\", m2)\n",
    "    lsh.insert(\"m3\", m3)\n",
    "    result = lsh.query(m1)\n",
    "    print(\"Approximate neighbours with weighted Jaccard similarity > 0.1\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    eg1()\n",
    "    eg2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(a, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit = []\n",
    "for i in range(len(X_train)):\n",
    "    curr = X_train[i].astype(str)\n",
    "    lit.append(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlist = []\n",
    "m = MinHash(num_perm=128)\n",
    "for i in range(len(lit)):\n",
    "        mlist.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
